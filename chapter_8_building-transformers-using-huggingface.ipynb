{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:20:47.495280Z",
     "iopub.status.busy": "2025-02-02T13:20:47.494867Z",
     "iopub.status.idle": "2025-02-02T13:20:47.803307Z",
     "shell.execute_reply": "2025-02-02T13:20:47.801927Z",
     "shell.execute_reply.started": "2025-02-02T13:20:47.495234Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d5f2c2ee5b4d95832ec4cfd5221bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1508\\AppData\\Local\\anaconda31\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\1508\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d6c31386c745219956f2e8694e9ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980e2a56c98546639842bb34b61d9b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cb61d5d25046419bedf2808271d269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c089d374e5646b0b6f5dff09ce6ce4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:21:12.848240Z",
     "iopub.status.busy": "2025-02-02T13:21:12.847814Z",
     "iopub.status.idle": "2025-02-02T13:21:12.855068Z",
     "shell.execute_reply": "2025-02-02T13:21:12.853958Z",
     "shell.execute_reply.started": "2025-02-02T13:21:12.848206Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['humans', 'have', 'evolved', 'a', 'lot', 'with', 'time', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Humans have evolved a lot with time.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)  # Output: ['transform', '##ers', 'are', 'revolution', '##izing', 'nl', '##p', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:25:02.982289Z",
     "iopub.status.busy": "2025-02-02T13:25:02.981702Z",
     "iopub.status.idle": "2025-02-02T13:25:02.991364Z",
     "shell.execute_reply": "2025-02-02T13:25:02.990147Z",
     "shell.execute_reply.started": "2025-02-02T13:25:02.982237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 4286, 2031, 7964, 1037, 2843, 2007, 2051, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentence, return_tensors='pt')  # Returns PyTorch tensors\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:06:19.614479Z",
     "iopub.status.busy": "2025-02-02T13:06:19.614069Z",
     "iopub.status.idle": "2025-02-02T13:06:19.729329Z",
     "shell.execute_reply": "2025-02-02T13:06:19.728266Z",
     "shell.execute_reply.started": "2025-02-02T13:06:19.614446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(last_hidden_states.shape)  # Shape: [batch_size, sequence_length, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T13:06:57.837539Z",
     "iopub.status.busy": "2025-02-02T13:06:57.837135Z",
     "iopub.status.idle": "2025-02-02T13:07:02.083356Z",
     "shell.execute_reply": "2025-02-02T13:07:02.082226Z",
     "shell.execute_reply.started": "2025-02-02T13:06:57.837498Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8110669255256653\n",
      "Epoch 2, Loss: 0.5848608016967773\n",
      "Epoch 3, Loss: 0.43619900941848755\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load a pre-trained model with a classification head\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Example training loop (simplified)\n",
    "epochs = 3  # Define number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Dummy inputs and labels for demonstration purposes\n",
    "    inputs = {\n",
    "        'input_ids': torch.tensor([[101, 2023, 2003, 1037, 7099, 102]]),  # Example tokenized input\n",
    "        'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1]])  # Attention mask\n",
    "    }\n",
    "    labels = torch.tensor([1])  # Example label (e.g., positive sentiment)\n",
    "\n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Custom Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T11:22:12.716595Z",
     "iopub.status.busy": "2025-02-22T11:22:12.715367Z",
     "iopub.status.idle": "2025-02-22T11:22:12.730297Z",
     "shell.execute_reply": "2025-02-22T11:22:12.729043Z",
     "shell.execute_reply.started": "2025-02-22T11:22:12.716548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "class CustomTransformerConfig(PretrainedConfig):\n",
    "    def __init__(self, vocab_size=30522, hidden_size=768, num_hidden_layers=12, \n",
    "                 num_attention_heads=12, intermediate_size=3072, max_position_embeddings=512, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "\n",
    "class CustomTransformer(PreTrainedModel):\n",
    "    config_class = CustomTransformerConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, config.max_position_embeddings, config.hidden_size))\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=config.hidden_size,\n",
    "            nhead=config.num_attention_heads,\n",
    "            dim_feedforward=config.intermediate_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=config.num_hidden_layers)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Embedding + Positional Encoding\n",
    "        embeddings = self.embedding(input_ids) + self.positional_encoding[:, :input_ids.size(1), :]\n",
    "        \n",
    "        # Create causal mask for autoregressive decoding\n",
    "        seq_len = input_ids.size(1)\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(input_ids.device)\n",
    "        \n",
    "        # Pass through the transformer decoder\n",
    "        transformer_output = self.transformer_decoder(tgt=embeddings, memory=embeddings, tgt_mask=causal_mask)\n",
    "        \n",
    "        # Classification head\n",
    "        logits = self.classifier(transformer_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T11:22:15.211322Z",
     "iopub.status.busy": "2025-02-22T11:22:15.210384Z",
     "iopub.status.idle": "2025-02-22T11:22:15.221752Z",
     "shell.execute_reply": "2025-02-22T11:22:15.220500Z",
     "shell.execute_reply.started": "2025-02-22T11:22:15.211276Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 sentences.\n"
     ]
    }
   ],
   "source": [
    "#Preparing the Dataset\n",
    "with open('data.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Clean and tokenize the data\n",
    "dataset = [line.strip() for line in lines if line.strip()]\n",
    "print(f\"Loaded {len(dataset)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T11:22:17.857843Z",
     "iopub.status.busy": "2025-02-22T11:22:17.857430Z",
     "iopub.status.idle": "2025-02-22T11:22:19.137869Z",
     "shell.execute_reply": "2025-02-22T11:22:19.136664Z",
     "shell.execute_reply.started": "2025-02-22T11:22:17.857808Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f57d5adc3a7453c8b3dd98206913344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62b140a83da43f4a14760d853651502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510aa3e2246e4a2db67030a0099cbabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de43a11783804951801904e351d7649c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tokenization\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Train a tokenizer on your dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_tokens(['<NEW_TOKEN>'])  # Add custom tokens if needed\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_data = [tokenizer.encode(sentence, max_length=512, truncation=True, padding='max_length') for sentence in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T11:22:22.904307Z",
     "iopub.status.busy": "2025-02-22T11:22:22.903253Z",
     "iopub.status.idle": "2025-02-22T11:24:39.396374Z",
     "shell.execute_reply": "2025-02-22T11:24:39.395104Z",
     "shell.execute_reply.started": "2025-02-22T11:22:22.904265Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.416277885437012\n",
      "Epoch 2, Loss: 9.843629837036133\n",
      "Epoch 3, Loss: 9.438532829284668\n"
     ]
    }
   ],
   "source": [
    "#Setting Up the Training Loop\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# Convert tokenized data to tensors\n",
    "input_ids = torch.tensor(tokenized_data)\n",
    "labels = torch.tensor([sentence[1:] + [tokenizer.pad_token_id] for sentence in tokenized_data])  # Shifted labels\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(input_ids, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = CustomTransformer(CustomTransformerConfig())\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_input_ids, batch_labels in dataloader:\n",
    "        batch_input_ids, batch_labels = batch_input_ids.to(device), batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_input_ids)\n",
    "        loss = loss_fn(outputs.view(-1, outputs.size(-1)), batch_labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:03:45.382264Z",
     "iopub.status.busy": "2025-02-09T13:03:45.381708Z",
     "iopub.status.idle": "2025-02-09T13:03:47.444260Z",
     "shell.execute_reply": "2025-02-09T13:03:47.443105Z",
     "shell.execute_reply.started": "2025-02-09T13:03:45.382218Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./custom_transformer/tokenizer_config.json',\n",
       " './custom_transformer/special_tokens_map.json',\n",
       " './custom_transformer/vocab.txt',\n",
       " './custom_transformer/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./custom_transformer')\n",
    "tokenizer.save_pretrained('./custom_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:09:20.580450Z",
     "iopub.status.busy": "2025-02-09T13:09:20.577789Z",
     "iopub.status.idle": "2025-02-09T13:09:22.131011Z",
     "shell.execute_reply": "2025-02-09T13:09:22.129884Z",
     "shell.execute_reply.started": "2025-02-09T13:09:20.580381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('./custom_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-09T13:09:41.184195Z",
     "iopub.status.busy": "2025-02-09T13:09:41.183798Z",
     "iopub.status.idle": "2025-02-09T13:09:41.213788Z",
     "shell.execute_reply": "2025-02-09T13:09:41.212530Z",
     "shell.execute_reply.started": "2025-02-09T13:09:41.184158Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./custom_transformer/tokenizer_config.json',\n",
       " './custom_transformer/special_tokens_map.json',\n",
       " './custom_transformer/vocab.txt',\n",
       " './custom_transformer/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./custom_transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case studies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:22:54.200539Z",
     "iopub.status.busy": "2025-02-22T12:22:54.200064Z",
     "iopub.status.idle": "2025-02-22T12:24:59.478158Z",
     "shell.execute_reply": "2025-02-22T12:24:59.476821Z",
     "shell.execute_reply.started": "2025-02-22T12:22:54.200503Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7d1cf2c056481f9f2b66e38b72a604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'t5-small': 'Summary Medical summary: Patient diagnosed with hypertension. Prescribed amlodipine. Follow-up in 2 weeks.',\n",
       " 't5-base': 'Amlodipine. Follow-up in 2 weeks.',\n",
       " 't5-large': 'diagnosed with hypertension. Prescribed amlodipine. Follow-up in 2 weeks. Medical summary: Patient diagnosed with hypertension',\n",
       " 'google/flan-t5-large': 'Patient diagnosed with hypertension. Prescribed amlodipine.',\n",
       " 'google/flan-t5-xl': 'Amlodipine is prescribed to a patient with hypertension.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-run the medical text summarization with different models after execution state reset\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# List of models to test\n",
    "models = [\n",
    "    \"t5-small\",\n",
    "    \"t5-base\",\n",
    "    \"t5-large\",\n",
    "    \"google/flan-t5-large\",\n",
    "    \"google/flan-t5-xl\"\n",
    "]\n",
    "\n",
    "# Example medical text to summarize\n",
    "input_text = \"Medical summary: Patient diagnosed with hypertension. Prescribed amlodipine. Follow-up in 2 weeks.\"\n",
    "\n",
    "# Store results\n",
    "summaries = {}\n",
    "\n",
    "for model_name in models:\n",
    "    try:\n",
    "        # Load tokenizer and model\n",
    "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "        # Tokenize input\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        summary_ids = model.generate(\n",
    "            input_ids, \n",
    "            max_length=30,  # Reduce output length for more concise summaries\n",
    "            num_beams=7,  # Increase beam search candidates\n",
    "            temperature=0.7,  # Reduce randomness\n",
    "            top_k=50,  # Increase diversity\n",
    "            top_p=0.9,  # Encourage more varied outputs\n",
    "            repetition_penalty=2.0,  # Stronger penalty to avoid repetition\n",
    "            early_stopping=True  # Stop when an optimal summary is found\n",
    "        )\n",
    "\n",
    "        # Decode output\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries[model_name] = summary\n",
    "\n",
    "    except Exception as e:\n",
    "        summaries[model_name] = f\"Error: {str(e)}\"\n",
    "\n",
    "summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T12:26:09.248513Z",
     "iopub.status.busy": "2025-02-22T12:26:09.248069Z",
     "iopub.status.idle": "2025-02-22T12:27:14.996217Z",
     "shell.execute_reply": "2025-02-22T12:27:14.995062Z",
     "shell.execute_reply.started": "2025-02-22T12:26:09.248475Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing BART model: facebook/bart-large-cnn\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671ec5cd0c214b0088670db8383fc543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107d6316efb548379a3484e7bb1e03a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bf796b2b96423fbe47d89d99768576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da580bca7674659aac3fb79aa9480d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7357b5d506de457fa4182e4154ab40a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5611be397857409088645056f58238d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1399: UserWarning: Unfeasible length constraints: `min_length` (56) is larger than the maximum possible length (30). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PEGASUS model: google/pegasus-xsum\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e78c24f01b40c98e6ebc58f97279ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f428de7f528a45dbb75a5370658965a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384384880f40466b94f4d5fbf2ec13e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9b3a8d67ce4625922771e0a59764a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.52M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0957e10f4294e16ba0ce10e378169f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddaff3687b4413dbd3b7240e507d3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca2ecfd156c48a6802bf9ee9a087145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing PEGASUS model: google/pegasus-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1441c893e15c4fb68ccf085df1fb7c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2e2c0aa7a240e6b289a77488b2a3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb0e59949e44ea48a810f0a386c8d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d611bb51cee4729b20fde320146a099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b9e8ea59334d6cb33a3b1855688b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d3d9bfc499499cbd08724f931d695b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: facebook/bart-large-cnn\n",
      "Summary: Medical summary: Patient diagnosed with hypertension. Prescribed amlodipine. Follow-up in 2 weeks. Back to Mail Online\n",
      "\n",
      "Model: google/pegasus-xsum\n",
      "Summary: The use of amlodipine in the treatment of hypertension: a case report.\n",
      "\n",
      "Model: google/pegasus-large\n",
      "Summary: Follow-up in 2 weeks.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# List of models to test\n",
    "models = {\n",
    "    \"facebook/bart-large-cnn\": (\"BART\", BartTokenizer, BartForConditionalGeneration),\n",
    "    \"google/pegasus-xsum\": (\"PEGASUS\", PegasusTokenizer, PegasusForConditionalGeneration),\n",
    "    \"google/pegasus-large\": (\"PEGASUS\", PegasusTokenizer, PegasusForConditionalGeneration),\n",
    "}\n",
    "\n",
    "# Example medical text to summarize\n",
    "input_text = \"Medical summary: Patient diagnosed with hypertension. Prescribed amlodipine. Follow-up in 2 weeks.\"\n",
    "\n",
    "# Store results\n",
    "summaries = {}\n",
    "\n",
    "for model_name, (model_type, TokenizerClass, ModelClass) in models.items():\n",
    "    try:\n",
    "        print(f\"Testing {model_type} model: {model_name}\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        tokenizer = TokenizerClass.from_pretrained(model_name)\n",
    "        model = ModelClass.from_pretrained(model_name)\n",
    "\n",
    "        # Tokenize input\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"longest\").input_ids\n",
    "\n",
    "        # Generate summary with optimized parameters\n",
    "        summary_ids = model.generate(\n",
    "            input_ids, \n",
    "            max_length=30,  # Keep it short and precise\n",
    "            num_beams=7,  # Increase beam search candidates\n",
    "            temperature=0.7,  # Reduce randomness\n",
    "            top_k=50,  # Encourage more variety in selection\n",
    "            top_p=0.9,  # Encourage more diverse outputs\n",
    "            repetition_penalty=2.0,  # Avoid repeating words\n",
    "            early_stopping=True  # Stop when an optimal summary is found\n",
    "        )\n",
    "\n",
    "        # Decode output\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries[model_name] = summary\n",
    "\n",
    "    except Exception as e:\n",
    "        summaries[model_name] = f\"Error: {str(e)}\"\n",
    "\n",
    "# Print summaries\n",
    "for model, summary in summaries.items():\n",
    "    print(f\"\\nModel: {model}\\nSummary: {summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T13:21:37.172000Z",
     "iopub.status.busy": "2025-02-22T13:21:37.171540Z",
     "iopub.status.idle": "2025-02-22T13:21:37.750849Z",
     "shell.execute_reply": "2025-02-22T13:21:37.748845Z",
     "shell.execute_reply.started": "2025-02-22T13:21:37.171959Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Score: 0.9022848606109619\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Encode product descriptions\n",
    "product_1 = tokenizer(\"Wireless Bluetooth Headphones with Noise Cancellation\", return_tensors=\"pt\")\n",
    "product_2 = tokenizer(\"Bluetooth Earbuds with Active Noise Cancelling\", return_tensors=\"pt\")\n",
    "\n",
    "# Compute similarity score\n",
    "embedding_1 = model(**product_1).last_hidden_state.mean(dim=1)\n",
    "embedding_2 = model(**product_2).last_hidden_state.mean(dim=1)\n",
    "##similarity_score = (embedding_1 @ embedding_2.T).item()\n",
    "\n",
    "\n",
    "similarity_score = cosine_similarity(embedding_1, embedding_2).item()\n",
    "print(\"Cosine Similarity Score:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T13:53:05.941526Z",
     "iopub.status.busy": "2025-02-22T13:53:05.939672Z",
     "iopub.status.idle": "2025-02-22T13:53:07.219217Z",
     "shell.execute_reply": "2025-02-22T13:53:07.217798Z",
     "shell.execute_reply.started": "2025-02-22T13:53:05.941458Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Probability: 0.8868260383605957\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Use a pre-trained classification model\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Example transaction description\n",
    "transaction = \"Multiple high-value purchases from various other overseas locations in the last hour.\"\n",
    "inputs = tokenizer(transaction, return_tensors=\"pt\")\n",
    "\n",
    "# Predict fraud probability\n",
    "outputs = model(**inputs)\n",
    "fraud_prob = outputs.logits.softmax(dim=1)[0, 1].item()\n",
    "\n",
    "print(\"Fraud Probability:\", fraud_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands-On Code: End-to-End Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T14:09:46.138802Z",
     "iopub.status.busy": "2025-02-22T14:09:46.138216Z",
     "iopub.status.idle": "2025-02-22T14:09:52.385338Z",
     "shell.execute_reply": "2025-02-22T14:09:52.384012Z",
     "shell.execute_reply.started": "2025-02-22T14:09:46.138750Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a3196025c749dbbb3a3f79cb199844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafa6dd984404ff7809b65b3e47f6345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56722783478d49ddaa501ead00443fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcc6edf3b494b32adfa229b9d70e130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062cd2063a9e4b7e86ed778f8f77a21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ca034fd07e4cefa4dba9a879e8047b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95900c260f4a469eb68ffe4f92c15789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T14:10:22.862252Z",
     "iopub.status.busy": "2025-02-22T14:10:22.861810Z",
     "iopub.status.idle": "2025-02-22T14:11:32.650930Z",
     "shell.execute_reply": "2025-02-22T14:11:32.649352Z",
     "shell.execute_reply.started": "2025-02-22T14:10:22.862219Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf39b5887fb74329bf76698599253ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ec532d27f644f29f58cb56d2f67094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2add0dc18e4442bd992043cecab0724c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T14:12:24.415056Z",
     "iopub.status.busy": "2025-02-22T14:12:24.414516Z",
     "iopub.status.idle": "2025-02-22T14:12:24.834083Z",
     "shell.execute_reply": "2025-02-22T14:12:24.832685Z",
     "shell.execute_reply.started": "2025-02-22T14:12:24.415017Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T14:12:42.269652Z",
     "iopub.status.busy": "2025-02-22T14:12:42.269181Z",
     "iopub.status.idle": "2025-02-22T14:12:42.277109Z",
     "shell.execute_reply": "2025-02-22T14:12:42.275531Z",
     "shell.execute_reply.started": "2025-02-22T14:12:42.269612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T14:25:09.344190Z",
     "iopub.status.busy": "2025-02-22T14:25:09.343609Z",
     "iopub.status.idle": "2025-02-22T14:25:09.358062Z",
     "shell.execute_reply": "2025-02-22T14:25:09.356760Z",
     "shell.execute_reply.started": "2025-02-22T14:25:09.344134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Disable Weights & Biases API key prompt\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,  # Reduced batch size\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,  # Reduce epochs\n",
    "    fp16=True,  # Mixed precision training\n",
    "    logging_steps=500,  # Less frequent logging\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    gradient_accumulation_steps=2,  # Reduce GPU memory usage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T14:24:54.522046Z",
     "iopub.status.busy": "2025-02-22T14:24:54.521531Z",
     "iopub.status.idle": "2025-02-22T14:24:54.530455Z",
     "shell.execute_reply": "2025-02-22T14:24:54.528937Z",
     "shell.execute_reply.started": "2025-02-22T14:24:54.522005Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T14:25:13.875218Z",
     "iopub.status.busy": "2025-02-22T14:25:13.873792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  31/9375 05:23 < 28:58:43, 0.09 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6589908,
     "sourceId": 10643029,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
