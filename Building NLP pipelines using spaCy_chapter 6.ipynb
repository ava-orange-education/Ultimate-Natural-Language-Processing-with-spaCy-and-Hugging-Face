{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')  # Load a pre-trained English pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1edc56366f0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1edc56375f0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1edc7d8e6c0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1edc7ffc3d0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1edc7ff5750>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1edc7d8e810>)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1edc7d8e810>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if 'ner' is already in the pipeline\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    # Add the 'ner' component using its factory name\n",
    "    nlp.add_pipe('ner', last=True)\n",
    "nlp.remove_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1edc5668a70>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1edc7d332f0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1edc9cd0190>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1edc9cd8b90>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simpler pipeline that doesn't use all components\n",
    "simple_pipe = spacy.load(\n",
    " \"en_core_web_sm\", \n",
    " disable=[\"parser\", \"ner\"])\n",
    "simple_pipe.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "cats\n",
      "are\n",
      "running\n",
      "around\n",
      "the\n",
      "house\n",
      ".\n",
      "the\n",
      "cat\n",
      "be\n",
      "run\n",
      "around\n",
      "the\n",
      "house\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"The cats are running around the house.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Tokenization\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "# Lemmatization\n",
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET\n",
      "cats NOUN\n",
      "are AUX\n",
      "running VERB\n",
      "around ADP\n",
      "the DET\n",
      "house NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last month's DATE\n",
      "Chinese NORP\n",
      "5% PERCENT\n",
      "Beijing GPE\n",
      "China GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = \"Although last month's measures were mainly targeted at making it easier for middle-class Chinese families to buy homes, analysts still believe the country's 5% growth target is too optimistic. Beijing's central bank has cut interest rates on mortgages and lowered the amount of money lenders must keep on hand. However, experts have warned more substantial reforms are needed if China wants to boost its property sector following the downfall of development firms like Evergrande.\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det\n",
      "cats nsubj\n",
      "are aux\n",
      "running ROOT\n",
      "around prep\n",
      "the det\n",
      "house pobj\n",
      ". punct\n"
     ]
    }
   ],
   "source": [
    "text = \"The cats are running around the house.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (17340, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiments</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>cleaned_review_length</th>\n",
       "      <th>review_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>i wish would have gotten one earlier love it a...</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>i ve learned this lesson again open the packag...</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>it is so slow and lags find better option</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>roller ball stopped working within months of m...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>i like the color and size but it few days out ...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiments                                     cleaned_review  \\\n",
       "0   positive  i wish would have gotten one earlier love it a...   \n",
       "1    neutral  i ve learned this lesson again open the packag...   \n",
       "2    neutral          it is so slow and lags find better option   \n",
       "3    neutral  roller ball stopped working within months of m...   \n",
       "4    neutral  i like the color and size but it few days out ...   \n",
       "\n",
       "   cleaned_review_length  review_score  \n",
       "0                     19             5  \n",
       "1                     88             1  \n",
       "2                      9             2  \n",
       "3                     12             1  \n",
       "4                     21             1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading Data\n",
    "\n",
    "df_chatgpt_sent_analysis = pd.read_csv(\"amazon_cleaned_reviews.csv\")\n",
    "\n",
    "print(f'Shape of data: {df_chatgpt_sent_analysis.shape}')\n",
    "# Show top 5 records\n",
    "df_chatgpt_sent_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17340 entries, 0 to 17339\n",
      "Data columns (total 4 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   sentiments             17340 non-null  object\n",
      " 1   cleaned_review         17337 non-null  object\n",
      " 2   cleaned_review_length  17340 non-null  int64 \n",
      " 3   review_score           17340 non-null  int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 542.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#Data Information\n",
    "\n",
    "df_chatgpt_sent_analysis.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the Text\n",
    "\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Create our list of punchuationmarks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stop words\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vector\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenzer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    \"\"\"This function will accepts a sentence as input and processes the sentence into tokens, performing lemmatization, \n",
    "    lowercasing, removing stop words and punctuations.\"\"\"\n",
    "    \n",
    "    # Creating our token object which is used to create documents with linguistic annotations\n",
    "    mytokens = parser(sentence)\n",
    "    \n",
    "    # lemmatizing each token and converting each token in lower case\n",
    "    # Note that spaCy uses '-PRON-' as lemma for all personal pronouns lkike me, I etc\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    \n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations]\n",
    "    \n",
    "    # Return preprocessed list of tokens\n",
    "    return mytokens    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "\n",
    "\n",
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        \"\"\"Override the transform method to clean text\"\"\"\n",
    "        return [clean_text(text) for text in X]\n",
    "    \n",
    "    def fit(self, X, y= None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep= True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    \"\"\"Removing spaces and converting the text into lowercase\"\"\"\n",
    "    return text.strip().lower()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering (Vectorization)\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range = (1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chatgpt_sent_analysis['review_score'] = df_chatgpt_sent_analysis['review_score'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dimension: (12138,)\n",
      "y_train dimension: (12138,)\n",
      "X_test dimension: (5202,)\n",
      "y_train dimension: (5202,)\n"
     ]
    }
   ],
   "source": [
    "#Create Train and Test Datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_chatgpt_sent_analysis['cleaned_review'] # The features we want to analyse\n",
    "ylabels = df_chatgpt_sent_analysis['review_score'] # There are labels where we have good, bad and neutral\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size = 0.3, random_state = 1)\n",
    "print(f'X_train dimension: {X_train.shape}')\n",
    "print(f'y_train dimension: {y_train.shape}')\n",
    "print(f'X_test dimension: {X_test.shape}')\n",
    "print(f'y_train dimension: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 4.31\n"
     ]
    }
   ],
   "source": [
    "# Sentence Embeddings using Spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the sentence embedding for \"This is a sample sentence.\"\n",
    "doc = nlp(\"This is a sample sentence.\")\n",
    "sentence_vector = doc.vector\n",
    "\n",
    "# Compute similarity between two sentences\n",
    "sent1 = nlp(\"This is a big book.\")\n",
    "sent2 = nlp(\"Here is another book but of red cover unlike the previous blue covered book.\")\n",
    "similarity = sent1.vector.dot(sent2.vector)\n",
    "print(f\"Similarity score: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Apple is looking at buying U.K. startup for $1 billion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Doc object\n",
    "document = simple_pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lemmatized strings\n",
    "cleaned_lemmas = [token.lemma_ for token in document if not \n",
    "                  token.is_space | token.is_punct | token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple', 'look', 'buy', 'U.K.', 'startup', '$', '1', 'billion']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1508\\AppData\\Local\\anaconda31\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"I like Delhi and Mumbai.\" with entities \"[(7, 11, 'LOC'), (17, 22, 'LOC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\1508\\AppData\\Local\\anaconda31\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Who is iphone from Apple?\" with entities \"[(19, 23, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at epoch 0: {'ner': 8.399999246001244}\n",
      "Losses at epoch 1: {'ner': 8.124842956662178}\n",
      "Losses at epoch 2: {'ner': 7.786208093166351}\n",
      "Losses at epoch 3: {'ner': 7.544182971119881}\n",
      "Losses at epoch 4: {'ner': 7.04532191157341}\n",
      "Losses at epoch 5: {'ner': 6.413692831993103}\n",
      "Losses at epoch 6: {'ner': 5.858181998133659}\n",
      "Losses at epoch 7: {'ner': 5.725336492061615}\n",
      "Losses at epoch 8: {'ner': 4.8050887286663055}\n",
      "Losses at epoch 9: {'ner': 4.566746324300766}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "# Sample training data\n",
    "TRAIN_DATA = [\n",
    "    (\"Who is iphone from Apple?\", {\"entities\": [(19, 23, \"ORG\")]}),\n",
    "    (\"I like Delhi and Mumbai.\", {\"entities\": [(7, 11, \"LOC\"), (17, 22, \"LOC\")]}),\n",
    "]\n",
    "\n",
    "# Load a blank model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create a new NER component and add it to the pipeline\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "\n",
    "# Add labels to the NER component\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Disable other pipeline components during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp.begin_training()\n",
    "    for epoch in range(10):\n",
    "        losses = {}\n",
    "        # Shuffle the training data\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        # Create batches and iterate over them\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            examples = [Example.from_dict(nlp.make_doc(text), ann) for text, ann in zip(texts, annotations)]\n",
    "            nlp.update(examples, drop=0.5, losses=losses)\n",
    "        print(f\"Losses at epoch {epoch}: {losses}\")\n",
    "\n",
    "# Save the trained model\n",
    "nlp.to_disk(\"custom_ner_model\")\n",
    "\n",
    "# Test the trained model\n",
    "test_text = \"Apple, Delhi sell more iphones than Apple, Mumbai.\"\n",
    "doc = nlp(test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a pre-existing model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Check if 'ner' is already in the pipeline\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    # Add the 'ner' component using its factory name\n",
    "    nlp.add_pipe('ner', last=True)\n",
    "\n",
    "# Now you can use the nlp object as usual\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
