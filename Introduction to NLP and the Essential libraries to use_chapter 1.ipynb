{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5bf4c56-dbdc-45b8-9596-d1e360118c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (80.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\1508\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\1508\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\1508\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\1508\\appdata\\local\\anaconda31\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c412da-293d-4aed-8bd3-fd7b5c61a429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.7 MB/s eta 0:00:08\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.4 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 2.0 MB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 2.1/12.8 MB 1.6 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.4 MB/s eta 0:00:08\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.4 MB/s eta 0:00:08\n",
      "     -------- ------------------------------- 2.6/12.8 MB 1.3 MB/s eta 0:00:08\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 2.9/12.8 MB 1.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.0 MB/s eta 0:00:10\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.0 MB/s eta 0:00:10\n",
      "     --------- ------------------------------ 3.1/12.8 MB 1.0 MB/s eta 0:00:10\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 923.6 kB/s eta 0:00:11\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 923.6 kB/s eta 0:00:11\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 923.6 kB/s eta 0:00:11\n",
      "     ----------- --------------------------- 3.7/12.8 MB 862.1 kB/s eta 0:00:11\n",
      "     ----------- --------------------------- 3.7/12.8 MB 862.1 kB/s eta 0:00:11\n",
      "     ----------- --------------------------- 3.7/12.8 MB 862.1 kB/s eta 0:00:11\n",
      "     ----------- --------------------------- 3.7/12.8 MB 862.1 kB/s eta 0:00:11\n",
      "     ----------- --------------------------- 3.9/12.8 MB 745.6 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 745.6 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 745.6 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 745.6 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 745.6 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 745.6 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 745.6 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 745.6 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.9/12.8 MB 745.6 kB/s eta 0:00:12\n",
      "     ------------ -------------------------- 4.2/12.8 MB 585.2 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.2/12.8 MB 585.2 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.2/12.8 MB 585.2 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.2/12.8 MB 585.2 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.2/12.8 MB 585.2 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.2/12.8 MB 585.2 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.2/12.8 MB 585.2 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.2/12.8 MB 585.2 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.2/12.8 MB 585.2 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.2/12.8 MB 585.2 kB/s eta 0:00:15\n",
      "     ------------ -------------------------- 4.2/12.8 MB 585.2 kB/s eta 0:00:15\n",
      "     ------------- ------------------------- 4.5/12.8 MB 455.0 kB/s eta 0:00:19\n",
      "     ------------- ------------------------- 4.5/12.8 MB 455.0 kB/s eta 0:00:19\n",
      "     ------------- ------------------------- 4.5/12.8 MB 455.0 kB/s eta 0:00:19\n",
      "     ------------- ------------------------- 4.5/12.8 MB 455.0 kB/s eta 0:00:19\n",
      "     ------------- ------------------------- 4.5/12.8 MB 455.0 kB/s eta 0:00:19\n",
      "     ------------- ------------------------- 4.5/12.8 MB 455.0 kB/s eta 0:00:19\n",
      "     -------------- ------------------------ 4.7/12.8 MB 426.3 kB/s eta 0:00:19\n",
      "     -------------- ------------------------ 4.7/12.8 MB 426.3 kB/s eta 0:00:19\n",
      "     -------------- ------------------------ 4.7/12.8 MB 426.3 kB/s eta 0:00:19\n",
      "     -------------- ------------------------ 4.7/12.8 MB 426.3 kB/s eta 0:00:19\n",
      "     --------------- ----------------------- 5.0/12.8 MB 418.3 kB/s eta 0:00:19\n",
      "     --------------- ----------------------- 5.0/12.8 MB 418.3 kB/s eta 0:00:19\n",
      "     --------------- ----------------------- 5.0/12.8 MB 418.3 kB/s eta 0:00:19\n",
      "     --------------- ----------------------- 5.0/12.8 MB 418.3 kB/s eta 0:00:19\n",
      "     --------------- ----------------------- 5.2/12.8 MB 414.0 kB/s eta 0:00:19\n",
      "     --------------- ----------------------- 5.2/12.8 MB 414.0 kB/s eta 0:00:19\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 413.7 kB/s eta 0:00:18\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 413.7 kB/s eta 0:00:18\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 413.7 kB/s eta 0:00:18\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 413.7 kB/s eta 0:00:18\n",
      "     ----------------- --------------------- 5.8/12.8 MB 412.5 kB/s eta 0:00:18\n",
      "     ----------------- --------------------- 5.8/12.8 MB 412.5 kB/s eta 0:00:18\n",
      "     ------------------ -------------------- 6.0/12.8 MB 412.9 kB/s eta 0:00:17\n",
      "     ------------------ -------------------- 6.0/12.8 MB 412.9 kB/s eta 0:00:17\n",
      "     ------------------ -------------------- 6.0/12.8 MB 412.9 kB/s eta 0:00:17\n",
      "     ------------------ -------------------- 6.0/12.8 MB 412.9 kB/s eta 0:00:17\n",
      "     ------------------- ------------------- 6.3/12.8 MB 410.9 kB/s eta 0:00:16\n",
      "     ------------------- ------------------- 6.3/12.8 MB 410.9 kB/s eta 0:00:16\n",
      "     ------------------- ------------------- 6.3/12.8 MB 410.9 kB/s eta 0:00:16\n",
      "     ------------------- ------------------- 6.6/12.8 MB 409.6 kB/s eta 0:00:16\n",
      "     ------------------- ------------------- 6.6/12.8 MB 409.6 kB/s eta 0:00:16\n",
      "     ------------------- ------------------- 6.6/12.8 MB 409.6 kB/s eta 0:00:16\n",
      "     -------------------- ------------------ 6.8/12.8 MB 410.8 kB/s eta 0:00:15\n",
      "     -------------------- ------------------ 6.8/12.8 MB 410.8 kB/s eta 0:00:15\n",
      "     --------------------- ----------------- 7.1/12.8 MB 414.3 kB/s eta 0:00:14\n",
      "     --------------------- ----------------- 7.1/12.8 MB 414.3 kB/s eta 0:00:14\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 418.3 kB/s eta 0:00:14\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 418.3 kB/s eta 0:00:14\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 418.3 kB/s eta 0:00:14\n",
      "     ----------------------- --------------- 7.6/12.8 MB 420.9 kB/s eta 0:00:13\n",
      "     ----------------------- --------------- 7.9/12.8 MB 426.0 kB/s eta 0:00:12\n",
      "     ----------------------- --------------- 7.9/12.8 MB 426.0 kB/s eta 0:00:12\n",
      "     ------------------------ -------------- 8.1/12.8 MB 429.8 kB/s eta 0:00:11\n",
      "     ------------------------ -------------- 8.1/12.8 MB 429.8 kB/s eta 0:00:11\n",
      "     ------------------------ -------------- 8.1/12.8 MB 429.8 kB/s eta 0:00:11\n",
      "     ------------------------- ------------- 8.4/12.8 MB 431.2 kB/s eta 0:00:11\n",
      "     ------------------------- ------------- 8.4/12.8 MB 431.2 kB/s eta 0:00:11\n",
      "     ------------------------- ------------- 8.4/12.8 MB 431.2 kB/s eta 0:00:11\n",
      "     -------------------------- ------------ 8.7/12.8 MB 431.9 kB/s eta 0:00:10\n",
      "     -------------------------- ------------ 8.7/12.8 MB 431.9 kB/s eta 0:00:10\n",
      "     --------------------------- ----------- 8.9/12.8 MB 433.9 kB/s eta 0:00:09\n",
      "     --------------------------- ----------- 8.9/12.8 MB 433.9 kB/s eta 0:00:09\n",
      "     --------------------------- ----------- 9.2/12.8 MB 437.4 kB/s eta 0:00:09\n",
      "     --------------------------- ----------- 9.2/12.8 MB 437.4 kB/s eta 0:00:09\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 441.2 kB/s eta 0:00:08\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 441.2 kB/s eta 0:00:08\n",
      "     ----------------------------- --------- 9.7/12.8 MB 445.4 kB/s eta 0:00:07\n",
      "     ----------------------------- --------- 9.7/12.8 MB 445.4 kB/s eta 0:00:07\n",
      "     ----------------------------- -------- 10.0/12.8 MB 449.8 kB/s eta 0:00:07\n",
      "     ------------------------------ ------- 10.2/12.8 MB 455.0 kB/s eta 0:00:06\n",
      "     ------------------------------ ------- 10.2/12.8 MB 455.0 kB/s eta 0:00:06\n",
      "     ------------------------------- ------ 10.5/12.8 MB 459.8 kB/s eta 0:00:06\n",
      "     ------------------------------- ------ 10.5/12.8 MB 459.8 kB/s eta 0:00:06\n",
      "     ------------------------------- ------ 10.7/12.8 MB 464.1 kB/s eta 0:00:05\n",
      "     -------------------------------- ----- 11.0/12.8 MB 467.3 kB/s eta 0:00:04\n",
      "     -------------------------------- ----- 11.0/12.8 MB 467.3 kB/s eta 0:00:04\n",
      "     -------------------------------- ----- 11.0/12.8 MB 467.3 kB/s eta 0:00:04\n",
      "     --------------------------------- ---- 11.3/12.8 MB 469.5 kB/s eta 0:00:04\n",
      "     --------------------------------- ---- 11.3/12.8 MB 469.5 kB/s eta 0:00:04\n",
      "     ---------------------------------- --- 11.5/12.8 MB 472.4 kB/s eta 0:00:03\n",
      "     ---------------------------------- --- 11.5/12.8 MB 472.4 kB/s eta 0:00:03\n",
      "     ----------------------------------- -- 11.8/12.8 MB 474.1 kB/s eta 0:00:03\n",
      "     ----------------------------------- -- 11.8/12.8 MB 474.1 kB/s eta 0:00:03\n",
      "     ----------------------------------- -- 12.1/12.8 MB 476.9 kB/s eta 0:00:02\n",
      "     ------------------------------------ - 12.3/12.8 MB 479.9 kB/s eta 0:00:02\n",
      "     ------------------------------------ - 12.3/12.8 MB 479.9 kB/s eta 0:00:02\n",
      "     -------------------------------------  12.6/12.8 MB 483.5 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 483.5 kB/s eta 0:00:01\n",
      "     -------------------------------------- 12.8/12.8 MB 486.0 kB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000195F27305C0>, 'Connection to release-assets.githubusercontent.com timed out. (connect timeout=15)')': /github-production-release-asset/84940268/15132ab6-4050-4914-8fe8-ac2c2fdcb9cf?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-20T06%3A24%3A05Z&rscd=attachment%3B+filename%3Den_core_web_sm-3.8.0-py3-none-any.whl&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-20T05%3A23%3A42Z&ske=2025-09-20T06%3A24%3A05Z&sks=b&skv=2018-11-09&sig=RMkPBC%2BdjtW%2B%2FfZwnYOViI1YhMCMrnJor%2Bh4WBNKYhQ%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1ODM0NzY3OCwibmJmIjoxNzU4MzQ3Mzc4LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.O1PuwFrD77o5WGsH_88WM43pLJzNDuT5pAOR2eCPjxk&response-content-disposition=attachment%3B%20filename%3Den_core_web_sm-3.8.0-py3-none-any.whl&response-content-type=application%2Foctet-stream\n",
      "  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x00000195EFAD14F0>, 'Connection to release-assets.githubusercontent.com timed out. (connect timeout=15)')': /github-production-release-asset/84940268/15132ab6-4050-4914-8fe8-ac2c2fdcb9cf?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-09-20T06%3A24%3A05Z&rscd=attachment%3B+filename%3Den_core_web_sm-3.8.0-py3-none-any.whl&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-09-20T05%3A23%3A42Z&ske=2025-09-20T06%3A24%3A05Z&sks=b&skv=2018-11-09&sig=RMkPBC%2BdjtW%2B%2FfZwnYOViI1YhMCMrnJor%2Bh4WBNKYhQ%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1ODM0NzY3OCwibmJmIjoxNzU4MzQ3Mzc4LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.O1PuwFrD77o5WGsH_88WM43pLJzNDuT5pAOR2eCPjxk&response-content-disposition=attachment%3B%20filename%3Den_core_web_sm-3.8.0-py3-none-any.whl&response-content-type=application%2Foctet-stream\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d1e8e92-1771-496a-b38a-7b7c8611c0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "best\n",
      "video\n",
      "game\n",
      "is\n",
      "Call\n",
      "of\n",
      "Duty\n",
      ",\n",
      "World\n",
      "War\n",
      "2\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"The best video game is Call of Duty, World War 2.\"\n",
    "tokens = nlp(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb43463-e2f4-439f-adb6-6fe3ea6fce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best video game is Call of Duty, World War 2.\n"
     ]
    }
   ],
   "source": [
    "text = \"The best video game is Call of Duty, World War 2.\"\n",
    "tokens = nlp(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f333180a-95c9-42ce-b95e-bcccae594a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars shift insurance liability toward manufacturers\n",
      "Autonomous ADJ amod\n",
      "cars NOUN nsubj\n",
      "shift VERB ROOT\n",
      "insurance NOUN compound\n",
      "liability NOUN dobj\n",
      "toward ADP prep\n",
      "manufacturers NOUN pobj\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sentences[1])\n",
    "print(doc.text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf89089d-4e72-493a-bbe3-8a2ea6e87d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\1508\\appdata\\local\\anaconda30\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\1508\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\1508\\appdata\\local\\anaconda30\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\1508\\appdata\\local\\anaconda30\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\1508\\appdata\\local\\anaconda30\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\1508\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\1508\\appdata\\local\\anaconda30\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\1508\\appdata\\local\\anaconda30\\lib\\site-packages)\n",
      "WARNING: Error parsing requirements for pyqt5-sip: [Errno 2] No such file or directory: 'c:\\\\users\\\\1508\\\\appdata\\\\local\\\\anaconda30\\\\lib\\\\site-packages\\\\PyQt5_sip-12.11.0.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for pyqtwebengine: [Errno 2] No such file or directory: 'c:\\\\users\\\\1508\\\\appdata\\\\local\\\\anaconda30\\\\lib\\\\site-packages\\\\PyQtWebEngine-5.15.4.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for sip: [Errno 2] No such file or directory: 'c:\\\\users\\\\1508\\\\appdata\\\\local\\\\anaconda30\\\\lib\\\\site-packages\\\\sip-6.6.2.dist-info\\\\METADATA'\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\1508\\appdata\\local\\anaconda30\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\1508\\appdata\\local\\anaconda30\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95efb5-25de-4813-87a5-9a0c2caa9bdb",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8236491-d931-45bf-81e7-6af02c0e6591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there be five element that make up life .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def stem_text(text):\n",
    "    doc = nlp(text)\n",
    "    stemmed_words = [token.lemma_.lower() for token in doc]  # Use lemma_ for base form\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "text = \"There are five elements that make up life.\"\n",
    "stemmed_text = stem_text(text)\n",
    "print(stemmed_text)  # Output: running race is a fun activit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d2b323c-e36e-4d55-a240-6f2652630ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['running', 'jumps', 'happily', 'borrow', 'begin']\n",
      "Stemmed words: ['run', 'jump', 'happili', 'borrow', 'begin']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example words for stemming\n",
    "words = [\"running\", \"jumps\", \"happily\", \"borrow\", \"begin\"]\n",
    " \n",
    "# Apply stemming to each word\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    " \n",
    "# Print the results\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6f13a-8215-4e49-bf9f-0a8112dcc4a5",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc2a9b13-4794-42e7-820b-8b41832a916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there be five element that make up life .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"There are five elements that make up life.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "lemmatized_words = [token.lemma_.lower() for token in doc]\n",
    "lemmatized_text = \" \".join(lemmatized_words)\n",
    "print(lemmatized_text)  # Output: running race is a fun activity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4abe9e1-12c7-41a5-8a46-d8c5adb3e545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxes : box\n",
      "universal : universal\n",
      "anger : angry\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#nltk.download('punkt')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"boxes :\", lemmatizer.lemmatize(\"boxes\"))\n",
    "print(\"universal :\", lemmatizer.lemmatize(\"universal\"))\n",
    " \n",
    "# a denotes adjective in \"pos\"\n",
    "print(\"anger :\", lemmatizer.lemmatize(\"angry\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "305fdee1-f62f-458d-9bb6-7fadc08f68af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords removal\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text for token in doc if token.text not in stop_words]\n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52715959-9eff-4d6a-b8b0-e1d5ac745d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing important sub - field data landscape .\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing is a very important sub-field in data landscape.\"\n",
    "filtered_text = remove_stopwords(text)\n",
    "print(filtered_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f4f461e-7e65-4711-a556-6fd58ad1addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing important sub - field data landscape .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Natural Language Processing is a very important sub-field in data landscape.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
    "filtered_text = \" \".join(filtered_tokens)\n",
    "print(filtered_text) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a38f7",
   "metadata": {},
   "source": [
    "#### Lowercasing, punctuation removal and accent removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d40ee70-47e5-4978-bdc2-dc7024aea839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cafe coffee and more\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"Café, Coffee, and More!\"\n",
    "\n",
    "# Process text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Lowercasing, punctuation removal, and accent removal\n",
    "cleaned_text = \" \".join(unidecode(token.text.lower()) for token in doc if token.text not in string.punctuation)\n",
    "\n",
    "print(cleaned_text)  # Output: \"cafe coffee and more\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7104262",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\1508/nltk_data'\n    - 'c:\\\\Users\\\\1508\\\\AppData\\\\Local\\\\anaconda31\\\\nltk_data'\n    - 'c:\\\\Users\\\\1508\\\\AppData\\\\Local\\\\anaconda31\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\1508\\\\AppData\\\\Local\\\\anaconda31\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\1508\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mCafé, Coffee, and More!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Tokenize text\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m tokens = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Lowercasing, punctuation removal, and accent removal\u001b[39;00m\n\u001b[32m     12\u001b[39m cleaned_text = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(unidecode(word.lower()) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string.punctuation)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\1508\\AppData\\Local\\anaconda31\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\1508\\AppData\\Local\\anaconda31\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\1508\\AppData\\Local\\anaconda31\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\1508\\AppData\\Local\\anaconda31\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\1508\\AppData\\Local\\anaconda31\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\1508\\AppData\\Local\\anaconda31\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\1508/nltk_data'\n    - 'c:\\\\Users\\\\1508\\\\AppData\\\\Local\\\\anaconda31\\\\nltk_data'\n    - 'c:\\\\Users\\\\1508\\\\AppData\\\\Local\\\\anaconda31\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\1508\\\\AppData\\\\Local\\\\anaconda31\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\1508\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Sample text\n",
    "text = \"Café, Coffee, and More!\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Lowercasing, punctuation removal, and accent removal\n",
    "cleaned_text = \" \".join(unidecode(word.lower()) for word in tokens if word not in string.punctuation)\n",
    "\n",
    "print(cleaned_text)  # Output: \"cafe coffee and more\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30593c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
